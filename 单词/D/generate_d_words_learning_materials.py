#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å­¦ä½è‹±è¯­è¯æ±‡å­¦ä¹ èµ„æ–™ç”Ÿæˆè„šæœ¬ - Då­—æ¯
åŠŸèƒ½ï¼šä»è¯æ±‡è¡¨ä¸­æå–Då¼€å¤´çš„å•è¯ï¼Œå¹¶æŒ‰å­¦ä½è‹±è¯­è¯æ±‡å­¦ä¹ æ¡ˆä¾‹ç¤ºèŒƒæ ¼å¼ç”Ÿæˆå­¦ä¹ èµ„æ–™
æ¯ä¸ªæ–‡ä»¶åŒ…å«50ä¸ªå•è¯ï¼ŒåŒ…å«6ä¸ªå­¦ä¹ æ¨¡å—ã€ä¾‹å¥è®¾è®¡ã€å­¦ä¹ è®°å½•è¡¨æ ¼å’Œæ€»ç»“
"""

import os
import re

# è®¾ç½®æ–‡ä»¶è·¯å¾„
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
VOCABULARY_FILE = os.path.join(os.path.dirname(BASE_DIR), 'å­¦ä½è‹±è¯­è¯æ±‡è¡¨_ä¼˜åŒ–ç‰ˆ.md')
OUTPUT_DIR = BASE_DIR

# åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
os.makedirs(OUTPUT_DIR, exist_ok=True)


def extract_words_from_file(file_path):
    """
    ä»æ–‡ä»¶ä¸­æå–Då¼€å¤´çš„å•è¯ï¼Œå¤„ç†å¤šè¡Œè¯æ€§å’Œé‡Šä¹‰çš„æƒ…å†µ
    """
    words = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
            # å…ˆæ‰¾åˆ°Då¼€å¤´å•è¯çš„éƒ¨åˆ†
            d_section_pattern = r'##\s+Då¼€å¤´å•è¯\s+((?:###.*?\n)+?)###\s+[A-Z]\w+'
            d_section_match = re.search(d_section_pattern, content, re.DOTALL)
            
            if d_section_match:
                d_section = d_section_match.group(1)
                
                # åŒ¹é…æ¯ä¸ªDå¼€å¤´çš„å•è¯ï¼ˆå¸¦éŸ³æ ‡å’Œè¯æ€§çš„ç¬¬ä¸€è¡Œï¼‰
                word_pattern = r'###\s+(D\w+?)\*?\[([^\]]+)\](\w+\.?)\s+(.+?)\n'
                word_matches = re.finditer(word_pattern, d_section)
                
                for match in word_matches:
                    word, phonetic, speech, meaning = match.groups()
                    
                    # æŸ¥æ‰¾è¯¥å•è¯å¯èƒ½çš„åç»­è¯æ€§å’Œé‡Šä¹‰
                    next_lines = d_section[match.end():]
                    additional_meanings = []
                    
                    # åŒ¹é…åç»­çš„è¯æ€§å’Œé‡Šä¹‰è¡Œ
                    additional_pattern = r'^###\s+([a-z]+\.?)\s+(.+?)\n'  # åŒ¹é…å¦‚ï¼š### n.æ—¥æŠ¥ è¿™æ ·çš„è¡Œ
                    additional_matches = re.finditer(additional_pattern, next_lines, re.MULTILINE)
                    
                    for add_match in additional_matches:
                        add_speech, add_meaning = add_match.groups()
                        additional_meanings.append(f"{add_speech} {add_meaning}")
                        # æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦è¿˜æ˜¯è¿™ä¸ªå•è¯çš„å†…å®¹
                        next_pos = add_match.end()
                        if next_pos < len(next_lines):
                            next_char = next_lines[next_pos]
                            if next_char == '#' and not next_lines[next_pos:next_pos+4].startswith('### '):
                                break  # ä¸æ˜¯###å¼€å¤´çš„è¡Œï¼Œç»“æŸæŸ¥æ‰¾
                        
                    # åˆå¹¶æ‰€æœ‰å«ä¹‰
                    full_meaning = meaning
                    if additional_meanings:
                        full_meaning += 'ï¼›' + 'ï¼›'.join(additional_meanings)
                    
                    words.append({
                        'word': word,
                        'phonetic': phonetic,
                        'speech': speech,
                        'meaning': full_meaning
                    })
        
        # å¦‚æœæ²¡æœ‰é€šè¿‡èŠ‚æ®µåŒ¹é…æ‰¾åˆ°è¶³å¤Ÿå•è¯ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•
        if not words or len(words) < 10:
            print(f"é€šè¿‡èŠ‚æ®µåŒ¹é…æå–åˆ° {len(words)} ä¸ªDå¼€å¤´çš„å•è¯ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•...")
            words = []
            
            # å¤‡ç”¨æ–¹æ³•ï¼šç›´æ¥åŒ¹é…æ‰€æœ‰Då¼€å¤´çš„å•è¯è¡Œï¼Œæ”¹è¿›æ­£åˆ™è¡¨è¾¾å¼ä»¥åŒ¹é…æ›´å¤šæ ¼å¼
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                # åŒ¹é…æ ¼å¼ï¼š### Dword[éŸ³æ ‡]... æ”¯æŒ*æ ‡è®°å’Œå„ç§æ ¼å¼å˜åŒ–
                pattern = r'###\s+(D\w+?)\*?\[([^\]]+)\]\s*([^\n]*)'
                matches = re.findall(pattern, content)
                
                # å»é™¤é‡å¤é¡¹ï¼ˆä½¿ç”¨é›†åˆå»é‡ï¼‰
                unique_words = {}
                
                for match in matches:
                    word, phonetic, rest = match
                    # ä»restä¸­æå–è¯æ€§å’Œå«ä¹‰
                    # å…ˆå»æ‰å¯èƒ½çš„æ¢è¡Œå’Œç©ºæ ¼
                    rest = rest.strip()
                    
                    # å¦‚æœrestä¸ºç©ºï¼Œå°è¯•ä»ä¸‹ä¸€è¡Œè·å–ä¿¡æ¯
                    if not rest:
                        # ç®€å•å¤„ç†ï¼Œå°†å«ä¹‰è®¾ä¸º"å¾…è¡¥å……"
                        speech = "n."  # é»˜è®¤è®¾ä¸ºåè¯
                        meaning = "å¾…è¡¥å……"
                    else:
                        # å°è¯•åŒ¹é…è¯æ€§å’Œå«ä¹‰ï¼Œè€ƒè™‘å¤šç§å¯èƒ½çš„æ ¼å¼
                        speech_meaning_pattern = r'(\w+\.?)\s+(.+)'
                        sm_match = re.match(speech_meaning_pattern, rest)
                        if sm_match:
                            speech, meaning = sm_match.groups()
                        else:
                            # å°è¯•å…¶ä»–å¯èƒ½çš„æ ¼å¼
                            speech = "n."  # é»˜è®¤è®¾ä¸ºåè¯
                            meaning = rest
                    
                    # å­˜å‚¨å•è¯ä¿¡æ¯ï¼Œé¿å…é‡å¤
                    if word not in unique_words:
                        unique_words[word] = {
                            'word': word,
                            'phonetic': phonetic,
                            'speech': speech,
                            'meaning': meaning
                        }
                
                # å°†å»é‡åçš„å•è¯æ·»åŠ åˆ°åˆ—è¡¨ä¸­
                words = list(unique_words.values())
        
        print(f"æˆåŠŸæå–åˆ° {len(words)} ä¸ªDå¼€å¤´çš„å•è¯")
        return words
    except Exception as e:
        print(f"æå–å•è¯æ—¶å‡ºé”™: {e}")
        return []


def generate_vocabulary_basic_info(word_info):
    """
    ç”Ÿæˆæ¨¡å—1ï¼šè¯æ±‡åŸºç¡€è§£æ
    """
    word = word_info['word']
    phonetic = word_info['phonetic']
    speech = word_info['speech']
    meaning = word_info['meaning']
    
    # è¯æ ¹è¯ç¼€æ‹†è§£ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
    root_affix = "**è¯æ ¹è¯ç¼€æ‹†è§£**ï¼š\n"
    root_affix += f"- {word} = æš‚æ— è¯¦ç»†è¯æ ¹è¯ç¼€ä¿¡æ¯\n"
    root_affix += f"- æ„è¯é€»è¾‘ï¼šæš‚æ— è¯¦ç»†æ„è¯é€»è¾‘ä¿¡æ¯\n"
    root_affix += f"- åŒæ ¹è¯ï¼šæš‚æ— åŒæ ¹è¯ä¿¡æ¯\n"
    
    # æ ¸å¿ƒä¿¡æ¯æ ‡å‡†åŒ–è¾“å‡º
    core_info = "\n**æ ¸å¿ƒä¿¡æ¯æ ‡å‡†åŒ–è¾“å‡º**ï¼š\n"
    core_info += f"- éŸ³æ ‡ï¼š[{phonetic}]\n"
    core_info += f"- è¯æ€§ï¼š{speech}\n"
    core_info += "- æ ¸å¿ƒå«ä¹‰ï¼š\n"
    
    # å¤„ç†å¤šä¸ªå«ä¹‰
    meanings = meaning.split('ï¼›')
    if len(meanings) > 0:
        core_info += f"  - â˜…â˜…â˜…â˜…â˜… {meanings[0]}\n"
    if len(meanings) > 1:
        for m in meanings[1:]:
            core_info += f"  - â˜…â˜…â˜…â˜…â˜† {m}\n"
    
    core_info += f"- è¯æºï¼šæš‚æ— è¯æºä¿¡æ¯\n"
    
    return f"### æ¨¡å—1ï¼šè¯æ±‡åŸºç¡€è§£æ\n\n{root_affix}{core_info}\n"


def generate_exam_focus(word_info):
    """
    ç”Ÿæˆæ¨¡å—2ï¼šè€ƒç‚¹èšç„¦
    """
    word = word_info['word']
    
    # é«˜é¢‘è€ƒç‚¹æ ‡æ³¨
    exam_focus = "**é«˜é¢‘è€ƒç‚¹æ ‡æ³¨**ï¼š\n"
    exam_focus += "- **è€ƒè¯•å¸¸è€ƒä¹‰é¡¹**ï¼š\n"
    exam_focus += "  - â˜…â˜…â˜…â˜…â˜… æ ¹æ®ä¸Šä¸‹æ–‡ç†è§£å•è¯åœ¨å¥å­ä¸­çš„å…·ä½“å«ä¹‰\n"
    exam_focus += "  - â˜…â˜…â˜…â˜…â˜† æŒæ¡å•è¯çš„å¸¸è§æ­é…å’Œå›ºå®šç”¨æ³•\n"
    
    exam_focus += "- **é«˜é¢‘é¢˜å‹å…³è”**ï¼š\n"
    exam_focus += "  - é˜…è¯»ç†è§£å æ¯”çº¦40%-60%\n"
    exam_focus += "  - å®Œå½¢å¡«ç©ºå æ¯”çº¦20%-30%\n"
    
    # å¿…è®°æ­é…
    exam_focus += "- **å¿…è®°æ­é…**ï¼š\n"
    if word == "do":
        exam_focus += "  - ğŸ“Œ do homeworkï¼šåšä½œä¸š\n"
        exam_focus += "    ä¾‹å¥æ¡†æ¶ï¼šI need to do my homework.\n"
        exam_focus += "  - ğŸ“Œ do some shoppingï¼šè´­ç‰©\n"
        exam_focus += "    ä¾‹å¥æ¡†æ¶ï¼šLet's do some shopping.\n"
    elif word == "date":
        exam_focus += "  - ğŸ“Œ date with sb.ï¼šä¸æŸäººçº¦ä¼š\n"
        exam_focus += "    ä¾‹å¥æ¡†æ¶ï¼šShe has a date with him.\n"
    elif word == "day":
        exam_focus += "  - ğŸ“Œ every dayï¼šæ¯å¤©\n"
        exam_focus += "    ä¾‹å¥æ¡†æ¶ï¼šI study English every day.\n"
        exam_focus += "  - ğŸ“Œ one dayï¼šæŸä¸€å¤©\n"
        exam_focus += "    ä¾‹å¥æ¡†æ¶ï¼šOne day, I will visit Paris.\n"
    elif word == "different":
        exam_focus += "  - ğŸ“Œ different fromï¼šä¸...ä¸åŒ\n"
        exam_focus += "    ä¾‹å¥æ¡†æ¶ï¼šMy book is different from yours.\n"
    elif word == "difficult":
        exam_focus += "  - ğŸ“Œ difficult to do sth.ï¼šåšæŸäº‹å›°éš¾\n"
        exam_focus += "    ä¾‹å¥æ¡†æ¶ï¼šIt's difficult to learn English.\n"
    else:
        exam_focus += f"  - ğŸ“Œ {word} + ç›¸å…³æ­é…ï¼šæš‚æ— ç‰¹å®šæ­é…ä¿¡æ¯\n"
        exam_focus += f"    ä¾‹å¥æ¡†æ¶ï¼šThe {word} is important.\n"
    
    # è€ƒè¯•é™·é˜±é¢„è­¦
    exam_tips = "\n**è€ƒè¯•é™·é˜±é¢„è­¦**ï¼š\n"
    exam_tips += f"- âš ï¸ æ³¨æ„{word}çš„æ­£ç¡®æ‹¼å†™å’Œå‘éŸ³\n"
    exam_tips += f"- âš ï¸ åœ¨ä¸åŒè¯­å¢ƒä¸­å¯èƒ½æœ‰ä¸åŒå«ä¹‰ï¼Œæ³¨æ„æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­\n"
    exam_tips += "- **ä¸Šä¸‹æ–‡åˆ¤æ–­æŠ€å·§**ï¼šç»“åˆå¥å­ç»“æ„å’Œä¸Šä¸‹æ–‡å†…å®¹ç†è§£è¯ä¹‰\n"
    
    return f"### æ¨¡å—2ï¼šè€ƒç‚¹èšç„¦\n\n{exam_focus}{exam_tips}\n"


def generate_memory_enhancement(word_info):
    """
    ç”Ÿæˆæ¨¡å—3ï¼šè®°å¿†å¼ºåŒ–
    """
    word = word_info['word']
    speech = word_info['speech']
    meaning = word_info['meaning']
    
    # è½»é‡åŒ–è®°å¿†æŠ€å·§
    memory_tips = "**è½»é‡åŒ–è®°å¿†æŠ€å·§**ï¼š\n"
    
    # æ ¹æ®å•è¯æä¾›æ›´å…·ä½“çš„è®°å¿†åœºæ™¯
    if word == "do":
        memory_tips += "- **åœºæ™¯ä¸²è”è®°å¿†**ï¼š\n"
        memory_tips += "  æƒ³è±¡è‡ªå·±å¯¹æœ‹å‹è¯´ï¼š'I do my homework every day.'ï¼ˆæˆ‘æ¯å¤©åšä½œä¸šï¼‰\n"
        memory_tips += "- **è°éŸ³è¾…åŠ©è®°å¿†**ï¼šæ— \n"
    elif word == "day":
        memory_tips += "- **åœºæ™¯ä¸²è”è®°å¿†**ï¼š\n"
        memory_tips += "  æƒ³è±¡è‡ªå·±æŒ‡ç€æ—¥å†è¯´ï¼š'Today is a sunny day.'ï¼ˆä»Šå¤©æ˜¯ä¸ªæ™´å¤©ï¼‰\n"
        memory_tips += "- **è°éŸ³è¾…åŠ©è®°å¿†**ï¼šæ— \n"
    elif word == "different":
        memory_tips += "- **åœºæ™¯ä¸²è”è®°å¿†**ï¼š\n"
        memory_tips += "  æƒ³è±¡è‡ªå·±æ¯”è¾ƒä¸¤ä¸ªç‰©å“è¯´ï¼š'These two books are different.'ï¼ˆè¿™ä¸¤æœ¬ä¹¦ä¸åŒï¼‰\n"
        memory_tips += "- **è°éŸ³è¾…åŠ©è®°å¿†**ï¼šæ— \n"
    else:
        memory_tips += "- **åœºæ™¯ä¸²è”è®°å¿†**ï¼š\n"
        memory_tips += f"  æƒ³è±¡åœ¨è‹±è¯­è¯¾å ‚ä¸Šï¼Œè€å¸ˆæ­£åœ¨è®²è§£å•è¯{word}ï¼Œå¹¶ç»™å‡ºäº†ä¾‹å¥\n"
        memory_tips += "- **è°éŸ³è¾…åŠ©è®°å¿†**ï¼šæš‚æ— åˆé€‚çš„è°éŸ³è®°å¿†æ³•\n"
    
    memory_tips += "- **å…³è”å·²çŸ¥è¯**ï¼šå°è¯•å°†è¯¥è¯ä¸å·²å­¦è¿‡çš„ç›¸å…³è¯æ±‡è”ç³»èµ·æ¥è®°å¿†\n"
    
    # æ˜“æ··è¯å¯¹æ¯”
    confusion_words = "\n**æ˜“æ··è¯å¯¹æ¯”**ï¼š\n\n"
    confusion_words += "| è¯æ±‡ | éŸ³æ ‡ | æ ¸å¿ƒåŒºåˆ«ï¼ˆè€ƒè¯•æ˜“è€ƒç‚¹ï¼‰ | çœŸé¢˜çº§ä¾‹å¥ |\n"
    confusion_words += "|------|------|-------------------------|------------|\n"
    
    if word == "do":
        confusion_words += f"| {word} | [{word_info['phonetic']}] | è¡¨ç¤ºåšã€å¹²çš„æ„æ€ | I do my best. |\n"
    elif word == "day":
        confusion_words += f"| {word} | [{word_info['phonetic']}] | è¡¨ç¤ºå¤©ã€æ—¥ | We have seven days in a week. |\n"
    else:
        confusion_words += f"| {word} | [{word_info['phonetic']}] | æš‚æ— è¯¦ç»†å¯¹æ¯”ä¿¡æ¯ | æš‚æ— è¯¦ç»†ä¾‹å¥ |\n"
    
    return f"### æ¨¡å—3ï¼šè®°å¿†å¼ºåŒ–\n\n{memory_tips}{confusion_words}\n"


def generate_grammar_usage(word_info):
    """
    ç”Ÿæˆæ¨¡å—4ï¼šè¯­æ³•ä¸åœºæ™¯åº”ç”¨
    """
    word = word_info['word']
    speech = word_info['speech']
    meaning = word_info['meaning']
    
    # é«˜é¢‘è¯çš„ç‰¹å®šä¾‹å¥
    specific_examples = {
        'do': [
            ("I do my homework every day.", "æˆ‘æ¯å¤©åšä½œä¸šã€‚", "doè¡¨ç¤ºåšã€å¹²"),
            ("Do you like English?", "ä½ å–œæ¬¢è‹±è¯­å—ï¼Ÿ", "doç”¨äºç–‘é—®å¥è¾…åŠ©åŠ¨è¯")
        ],
        'date': [
            ("Today is an important date.", "ä»Šå¤©æ˜¯ä¸ªé‡è¦çš„æ—¥æœŸã€‚", "dateè¡¨ç¤ºæ—¥æœŸ"),
            ("She has a date with him.", "å¥¹å’Œä»–æœ‰ä¸ªçº¦ä¼šã€‚", "dateè¡¨ç¤ºçº¦ä¼š")
        ],
        'day': [
            ("Today is a sunny day.", "ä»Šå¤©æ˜¯ä¸ªæ™´å¤©ã€‚", "dayè¡¨ç¤ºå¤©"),
            ("I study English every day.", "æˆ‘æ¯å¤©å­¦ä¹ è‹±è¯­ã€‚", "every dayè¡¨ç¤ºæ¯å¤©")
        ],
        'different': [
            ("These two books are different.", "è¿™ä¸¤æœ¬ä¹¦ä¸åŒã€‚", "differentè¡¨ç¤ºä¸åŒçš„"),
            ("My opinion is different from yours.", "æˆ‘çš„æ„è§å’Œä½ çš„ä¸åŒã€‚", "different fromè¡¨ç¤ºä¸...ä¸åŒ")
        ],
        'difficult': [
            ("English is not difficult for me.", "è‹±è¯­å¯¹æˆ‘æ¥è¯´ä¸éš¾ã€‚", "difficultè¡¨ç¤ºå›°éš¾çš„"),
            ("It's difficult to learn Chinese.", "å­¦ä¹ ä¸­æ–‡å¾ˆéš¾ã€‚", "difficult to doè¡¨ç¤ºåšæŸäº‹å›°éš¾")
        ],
        'doctor': [
            ("He is a good doctor.", "ä»–æ˜¯ä¸ªå¥½åŒ»ç”Ÿã€‚", "doctorè¡¨ç¤ºåŒ»ç”Ÿ"),
            ("You should see a doctor.", "ä½ åº”è¯¥å»çœ‹åŒ»ç”Ÿã€‚", "see a doctorè¡¨ç¤ºçœ‹åŒ»ç”Ÿ")
        ],
        'door': [
            ("Please open the door.", "è¯·å¼€é—¨ã€‚", "doorè¡¨ç¤ºé—¨"),
            ("The door is closed.", "é—¨æ˜¯å…³ç€çš„ã€‚", "closedæè¿°é—¨çš„çŠ¶æ€")
        ],
        'drink': [
            ("I drink water every day.", "æˆ‘æ¯å¤©å–æ°´ã€‚", "drinkè¡¨ç¤ºå–"),
            ("Would you like a drink?", "ä½ æƒ³å–ä¸€æ¯å—ï¼Ÿ", "drinkä½œä¸ºåè¯è¡¨ç¤ºé¥®æ–™")
        ],
        'drive': [
            ("He can drive a car.", "ä»–ä¼šå¼€è½¦ã€‚", "driveè¡¨ç¤ºé©¾é©¶"),
            ("Let's drive to the park.", "æˆ‘ä»¬å¼€è½¦å»å…¬å›­å§ã€‚", "drive toè¡¨ç¤ºå¼€è½¦å»æŸåœ°")
        ]
    }
    
    # ä¾‹å¥è®¾è®¡ä¸æ‹†è§£
    examples = "**ä¾‹å¥è®¾è®¡ä¸æ‹†è§£**ï¼š\n\n"
    examples += "| ä¾‹å¥å†…å®¹ | è¯æ±‡æ‹†åˆ† | è¯­æ³•æ ‡è®° | ç¿»è¯‘ | è¯­æ³•æç¤º |\n"
    examples += "|---------|---------|---------|------|---------|\n"
    
    # ç”Ÿæˆä¾‹å¥
    if word in specific_examples:
        # ä½¿ç”¨ç‰¹å®šä¾‹å¥
        for example_sentence, translation, grammar_tip in specific_examples[word][:2]:
            # ç®€å•çš„è¯æ±‡æ‹†åˆ†
            vocab_split = example_sentence.replace(word, f"{word} [{speech}] {meaning[:10]}...")
            grammar_mark = "ä¸€èˆ¬ç°åœ¨æ—¶" if 'is' in example_sentence or 'are' in example_sentence else "ç®€å•å¥"
            examples += f"| {example_sentence} | {vocab_split} | {grammar_mark} | {translation} | {grammar_tip} |\n"
    else:
        # ä¸ºå…¶ä»–å•è¯ç”Ÿæˆé€šç”¨ä¾‹å¥
        # æ ¹æ®è¯æ€§ç”Ÿæˆåˆé€‚çš„ä¾‹å¥
        if 'n.' in speech:
            example_sentence = f"The {word} is important."
            translation = f"è¿™ä¸ª{meaning.split('ï¼›')[0] if 'ï¼›' in meaning else meaning}å¾ˆé‡è¦ã€‚"
            grammar_tip = "ä½œä¸ºåè¯åœ¨å¥å­ä¸­ä½œä¸»è¯­"
        elif 'v.' in speech:
            example_sentence = f"We should {word} it carefully."
            translation = f"æˆ‘ä»¬åº”è¯¥ä»”ç»†åœ°{meaning.split('ï¼›')[0] if 'ï¼›' in meaning else meaning}å®ƒã€‚"
            grammar_tip = "ä½œä¸ºåŠ¨è¯åœ¨å¥å­ä¸­ä½œè°“è¯­"
        elif 'adj.' in speech:
            example_sentence = f"This is a {word} example."
            translation = f"è¿™æ˜¯ä¸€ä¸ª{meaning.split('ï¼›')[0] if 'ï¼›' in meaning else meaning}çš„ä¾‹å­ã€‚"
            grammar_tip = "ä½œä¸ºå½¢å®¹è¯ä¿®é¥°åè¯example"
        else:
            example_sentence = f"{word} is a keyword in this sentence."
            translation = f"åœ¨è¿™ä¸ªå¥å­ä¸­ï¼Œ{word}æ˜¯ä¸€ä¸ªå…³é”®è¯ã€‚"
            grammar_tip = "åœ¨å¥å­ä¸­ä½œä¸»è¯­"
        
        # ç®€å•çš„è¯æ±‡æ‹†åˆ†
        vocab_split = example_sentence.replace(word, f"{word} [{speech}] {meaning[:10]}...")
        grammar_mark = "ä¸€èˆ¬ç°åœ¨æ—¶ï¼Œç®€å•å¥"
        examples += f"| {example_sentence} | {vocab_split} | {grammar_mark} | {translation} | {grammar_tip} |\n"
    
    # é¢˜å‹é€‚é…æŠ€å·§
    skill_tips = "\n**é¢˜å‹é€‚é…æŠ€å·§**ï¼š\n"
    skill_tips += f"- **é’ˆå¯¹é˜…è¯»**ï¼šåœ¨é˜…è¯»ä¸­é‡åˆ°{word}æ—¶ï¼Œç»“åˆä¸Šä¸‹æ–‡ç†è§£å…¶å«ä¹‰\n"
    skill_tips += f"- **é’ˆå¯¹å®Œå½¢**ï¼šæ³¨æ„{word}çš„å›ºå®šæ­é…å’Œè¯­æ³•ç”¨æ³•\n"
    skill_tips += f"- **é’ˆå¯¹å†™ä½œ**ï¼šå°è¯•åœ¨å†™ä½œä¸­ä½¿ç”¨{word}ï¼Œæå‡è¡¨è¾¾å¤šæ ·æ€§\n"
    
    return f"### æ¨¡å—4ï¼šè¯­æ³•ä¸åœºæ™¯åº”ç”¨\n\n{examples}{skill_tips}\n"


def generate_self_test_review(word_info):
    """
    ç”Ÿæˆæ¨¡å—5ï¼šè‡ªæµ‹ä¸å¤ä¹ è§„åˆ’
    """
    word = word_info['word']
    
    # å³æ—¶è‡ªæµ‹é¢˜
    self_test = "**å³æ—¶è‡ªæµ‹é¢˜**ï¼š\n\n"
    
    # é€‰è¯å¡«ç©ºé¢˜
    self_test += "1. **é€‰è¯å¡«ç©º**ï¼š\n"
    if word == "do":
        self_test += "   I ____ my homework every day.\n"
        self_test += "   A. do  B. does  C. did\n"
        self_test += "   ç­”æ¡ˆï¼šA\n"
        self_test += "   è§£æï¼šä¸»è¯­æ˜¯Iï¼ŒåŠ¨è¯ç”¨åŸå½¢do\n"
    elif word == "day":
        self_test += "   There are seven ____ in a week.\n"
        self_test += "   A. day  B. days  C. dayes\n"
        self_test += "   ç­”æ¡ˆï¼šB\n"
        self_test += "   è§£æï¼šsevenè¡¨ç¤ºå¤æ•°ï¼Œdayçš„å¤æ•°æ˜¯days\n"
    elif word == "different":
        self_test += "   My book is ____ from yours.\n"
        self_test += "   A. different  B. same  C. difficult\n"
        self_test += "   ç­”æ¡ˆï¼šA\n"
        self_test += "   è§£æï¼šdifferent fromè¡¨ç¤ºä¸...ä¸åŒ\n"
    else:
        self_test += f"   We should remember the word ____.\n"
        self_test += f"   A. {word}  B. test  C. example\n"
        self_test += f"   ç­”æ¡ˆï¼šA\n"
        self_test += f"   è§£æï¼šæ ¹æ®å¥å­æ„æ€ï¼Œè¿™é‡Œéœ€è¦å¡«å…¥{word}\n"
    
    self_test += "\n"
    
    # è¯­æ³•åˆ¤æ–­é¢˜
    self_test += "2. **è¯­æ³•åˆ¤æ–­**ï¼š\n"
    if word == "do":
        self_test += "   å¥å­'She do her homework.'æ˜¯å¦æ­£ç¡®ï¼Ÿ\n"
        self_test += "   ç­”æ¡ˆï¼šä¸æ­£ç¡®\n"
        self_test += "   è§£æï¼šä¸»è¯­æ˜¯ç¬¬ä¸‰äººç§°å•æ•°ï¼Œåº”è¯¥ç”¨does\n"
    elif word == "day":
        self_test += "   å¥å­'One day, I will visit Beijing.'æ˜¯å¦æ­£ç¡®ï¼Ÿ\n"
        self_test += "   ç­”æ¡ˆï¼šæ­£ç¡®\n"
        self_test += "   è§£æï¼šone dayè¡¨ç¤ºå°†æ¥çš„æŸä¸€å¤©ï¼Œç”¨å°†æ¥æ—¶\n"
    else:
        self_test += f"   å¥å­'I {word} this book.'æ˜¯å¦æ­£ç¡®ï¼Ÿ\n"
        self_test += "   ç­”æ¡ˆï¼šéœ€è¦æ ¹æ®å…·ä½“è¯­å¢ƒåˆ¤æ–­\n"
        self_test += "   è§£æï¼šå¦‚æœ{word}æ˜¯åŠç‰©åŠ¨è¯ï¼Œè¿™ä¸ªå¥å­ç»“æ„æ­£ç¡®\n"
    
    # è‰¾å®¾æµ©æ–¯å¤ä¹ æç¤º
    review_plan = "\n**è‰¾å®¾æµ©æ–¯å¤ä¹ æç¤º**ï¼š\n"
    review_plan += "- **ç¬¬1æ¬¡å¤ä¹ **ï¼šå­¦å®Œå½“å¤©ç¡å‰ï¼ˆ5åˆ†é’Ÿï¼‰\n"
    review_plan += f"  é‡ç‚¹çœ‹ï¼šå¿…è®°æ­é… + {word}çš„æ ¸å¿ƒå«ä¹‰\n"
    review_plan += "- **ç¬¬2æ¬¡å¤ä¹ **ï¼šç¬¬2å¤©æ—©ä¸Šï¼ˆ3åˆ†é’Ÿï¼‰\n"
    review_plan += f"  å¿«é€Ÿè¿‡ï¼š{word}çš„æ‹¼å†™ã€å‘éŸ³å’Œä¸»è¦ä¾‹å¥\n"
    review_plan += "- **ç¬¬3æ¬¡å¤ä¹ **ï¼šç¬¬7å¤©ï¼ˆ5åˆ†é’Ÿï¼‰\n"
    review_plan += f"  é‡åšè‡ªæµ‹é¢˜ï¼Œæ£€æŸ¥æ˜¯å¦çœŸæ­£æŒæ¡{word}\n"
    
    return f"### æ¨¡å—5ï¼šè‡ªæµ‹ä¸å¤ä¹ è§„åˆ’\n\n{self_test}{review_plan}\n"


def generate_personalized_interaction(word_info):
    """
    ç”Ÿæˆæ¨¡å—6ï¼šä¸ªæ€§åŒ–äº¤äº’ç¤ºä¾‹
    """
    word = word_info['word']
    
    # ç”¨æˆ·æé—®å“åº”ç¤ºä¾‹
    qa_examples = "**ç”¨æˆ·æé—®å“åº”ç¤ºä¾‹**ï¼š\n\n"
    qa_examples += f"1. **ç”¨æˆ·é—®**ï¼š'{word}é‡ç‚¹è€ƒä»€ä¹ˆï¼Ÿ'\n"
    qa_examples += f"   **å“åº”**ï¼š{word}åœ¨å­¦ä½è‹±è¯­è€ƒè¯•ä¸­ä¸»è¦è€ƒæŸ¥å…¶åŸºæœ¬å«ä¹‰å’Œç”¨æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨é˜…è¯»ç†è§£å’Œå®Œå½¢å¡«ç©ºä¸­çš„åº”ç”¨ã€‚å»ºè®®é‡ç‚¹æŒæ¡å…¶æ ¸å¿ƒå«ä¹‰ã€å¸¸è§æ­é…å’Œè¯­æ³•ç”¨æ³•ã€‚\n\n"
    
    qa_examples += f"2. **ç”¨æˆ·é—®**ï¼š'æ€ä¹ˆè®°{word}è¿™ä¸ªè¯ï¼Ÿ'\n"
    qa_examples += f"   **å“åº”**ï¼šæ¨èç»“åˆåœºæ™¯è®°å¿†æ³•ï¼Œå°†{word}æ”¾åœ¨å…·ä½“çš„å¥å­æˆ–åœºæ™¯ä¸­è®°å¿†ã€‚åŒæ—¶ï¼Œå¯ä»¥é€šè¿‡è”æƒ³è®°å¿†æ³•ï¼Œå°†{word}ä¸å·²å­¦è¿‡çš„ç›¸å…³è¯æ±‡è”ç³»èµ·æ¥è®°å¿†ã€‚\n\n"
    
    qa_examples += f"3. **ç”¨æˆ·é—®**ï¼š'å®Œå½¢å¡«ç©ºæ€ä¹ˆç”¨{word}ï¼Ÿ'\n"
    qa_examples += f"   **å“åº”**ï¼šåœ¨å®Œå½¢å¡«ç©ºä¸­ï¼Œä½¿ç”¨{word}æ—¶éœ€è¦æ³¨æ„å…¶å›ºå®šæ­é…å’Œè¯­æ³•ç”¨æ³•ã€‚ç»“åˆä¸Šä¸‹æ–‡è¯­å¢ƒï¼Œåˆ¤æ–­{word}åœ¨å¥å­ä¸­çš„è¯æ€§å’Œå«ä¹‰ï¼Œé€‰æ‹©æ­£ç¡®çš„å½¢å¼ã€‚\n"
    
    # è¿›åº¦é€‚é…ç¤ºä¾‹
    progress_examples = "\n**è¿›åº¦é€‚é…ç¤ºä¾‹**ï¼š\n\n"
    progress_examples += f"1. **ç”¨æˆ·æåŠ**ï¼š'å·²å­¦è¿‡ç›¸å…³è¯æ±‡'\n"
    progress_examples += f"   **å“åº”**ï¼šåœ¨å­¦ä¹ {word}æ—¶ï¼Œå¯ä»¥ä¸»åŠ¨å…³è”å·²å­¦è¿‡çš„ç›¸å…³è¯æ±‡ï¼Œå¯¹æ¯”å®ƒä»¬çš„åŒºåˆ«å’Œè”ç³»ï¼Œå¸®åŠ©æ›´å¥½åœ°ç†è§£å’Œè®°å¿†ã€‚\n\n"
    
    progress_examples += f"2. **ç”¨æˆ·æåŠ**ï¼š'å¤‡è€ƒå€’è®¡æ—¶'\n"
    progress_examples += f"   **å“åº”**ï¼šæ ¹æ®å¤‡è€ƒæ—¶é—´ï¼Œè°ƒæ•´å¤ä¹ è®¡åˆ’ï¼Œé‡ç‚¹èšç„¦{word}çš„é«˜é¢‘è€ƒç‚¹å’Œæ ¸å¿ƒç”¨æ³•ï¼Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚\n"
    
    return f"### æ¨¡å—6ï¼šä¸ªæ€§åŒ–äº¤äº’ç¤ºä¾‹\n\n{qa_examples}{progress_examples}\n"


def generate_learning_materials(word_list, unit_num, total_units):
    """
    ç”Ÿæˆå®Œæ•´çš„å­¦ä¹ èµ„æ–™
    """
    content = "# Då­—æ¯å•è¯å­¦ä¹ èµ„æ–™_å•å…ƒ" + str(unit_num) + "\n\n"
    
    # æ·»åŠ å­¦ä¹ è¯´æ˜
    content += "## ğŸ“– å­¦ä¹ è¯´æ˜\n\n"
    content += "æœ¬èµ„æ–™æŒ‰ç…§å­¦ä½è‹±è¯­è¯æ±‡å­¦ä¹ æ¡ˆä¾‹ç¤ºèŒƒæ ¼å¼åˆ¶ä½œï¼ŒåŒ…å«å®Œæ•´çš„6ä¸ªå­¦ä¹ æ¨¡å—ã€‚\n"
    content += "è¯·æŒ‰ç…§ä»¥ä¸‹æ–¹æ³•ä½¿ç”¨æœ¬èµ„æ–™ï¼š\n"
    content += "1. æ¯å¤©å­¦ä¹ ä¸€å®šæ•°é‡çš„å•è¯\n"
    content += "2. æ¯ä¸ªå•è¯éƒ½åŒ…å«è¯¦ç»†çš„ä¾‹å¥å’Œç”¨æ³•è¯´æ˜\n"
    content += "3. å®Œæˆæ¯ä¸ªå•è¯çš„è‡ªæµ‹é¢˜\n"
    content += "4. æŒ‰ç…§è‰¾å®¾æµ©æ–¯å¤ä¹ è®¡åˆ’å®šæœŸå¤ä¹ \n"
    content += "5. åœ¨å­¦ä¹ è®°å½•è¡¨æ ¼ä¸­è®°å½•å­¦ä¹ æƒ…å†µ\n\n"
    
    # ä¸ºæ¯ä¸ªå•è¯ç”Ÿæˆå­¦ä¹ æ¡ˆä¾‹
    for i, word_info in enumerate(word_list):
        word = word_info['word']
        content += "## è¯æ±‡å­¦ä¹ æ¡ˆä¾‹ï¼š" + word + "\n\n"
        
        # ç”Ÿæˆå„ä¸ªæ¨¡å—
        content += generate_vocabulary_basic_info(word_info)
        content += "---\n\n"
        content += generate_exam_focus(word_info)
        content += "---\n\n"
        content += generate_memory_enhancement(word_info)
        content += "---\n\n"
        content += generate_grammar_usage(word_info)
        content += "---\n\n"
        content += generate_self_test_review(word_info)
        content += "---\n\n"
        content += generate_personalized_interaction(word_info)
        
        # åœ¨æœ€åä¸€ä¸ªå•è¯åä¸æ·»åŠ åˆ†éš”çº¿
        if i < len(word_list) - 1:
            content += "\n---\n\n"
    
    # æ·»åŠ å­¦ä¹ è®°å½•è¡¨æ ¼å’Œæ€»ç»“
    content += "\n---\n\n"
    content += "## ğŸ“Š å­¦ä¹ è®°å½•è¡¨æ ¼\n\n"
    content += "| å•è¯ | æŒæ¡ç¨‹åº¦ | å­¦ä¹ æ—¥æœŸ | å¤ä¹ æ¬¡æ•° | å¤‡æ³¨ |\n"
    content += "|------|----------|----------|----------|------|\n"
    for word_info in word_list:
        content += "| " + word_info['word'] + " | â­â­â­ | | 0 | |\n"
    
    content += "\n## ğŸ“ å­¦ä¹ æ€»ç»“\n\n"
    content += "- **æœ¬å•å…ƒå•è¯æ•°**ï¼š" + str(len(word_list)) + "ä¸ª\n"
    content += "- **å•å…ƒç¼–å·**ï¼šD" + str(unit_num) + "/" + str(total_units) + "\n"
    content += "- **å­¦ä¹ è¿›åº¦**ï¼š0%\n"
    content += "- **å·²æŒæ¡å•è¯**ï¼š0ä¸ª\n"
    content += "- **é‡ç‚¹å•è¯**ï¼šéœ€æ ¹æ®ä¸ªäººå­¦ä¹ æƒ…å†µæ ‡è®°\n\n"
    
    content += "### ğŸ’ª å­¦ä¹ é¼“åŠ±\n"
    content += "åšæŒæ¯å¤©å­¦ä¹ ï¼Œç›¸ä¿¡ä½ ä¸€å®šèƒ½å¤ŸæŒæ¡è¿™äº›å•è¯ï¼Œä¸ºå­¦ä½è‹±è¯­è€ƒè¯•æ‰“ä¸‹åšå®çš„åŸºç¡€ï¼"
    
    return content


def main():
    """
    ä¸»å‡½æ•°
    """
    # æå–å•è¯
    words = extract_words_from_file(VOCABULARY_FILE)
    if not words:
        print("æ²¡æœ‰æå–åˆ°ä»»ä½•å•è¯ï¼Œç¨‹åºé€€å‡ºã€‚")
        return
    
    # è®¡ç®—éœ€è¦ç”Ÿæˆçš„æ–‡ä»¶æ•°é‡
    words_per_file = 50
    total_files = (len(words) + words_per_file - 1) // words_per_file  # å‘ä¸Šå–æ•´
    
    # ç”Ÿæˆå¤šä¸ªå­¦ä¹ èµ„æ–™æ–‡ä»¶
    for i in range(total_files):
        start_idx = i * words_per_file
        end_idx = min((i + 1) * words_per_file, len(words))
        current_words = words[start_idx:end_idx]
        
        # ç”Ÿæˆå­¦ä¹ èµ„æ–™
        unit_num = i + 1
        content = generate_learning_materials(current_words, unit_num, total_files)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_file = os.path.join(OUTPUT_DIR, f"Då­—æ¯å•è¯å­¦ä¹ èµ„æ–™_å•å…ƒ{unit_num}.md")
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"å·²ç”Ÿæˆ: {output_file}")
    
    print(f"\næ‰€æœ‰Då­—æ¯å•è¯å­¦ä¹ èµ„æ–™å·²ç”Ÿæˆå®Œæˆï¼å…±ç”Ÿæˆ{total_files}ä¸ªæ–‡ä»¶ã€‚")


if __name__ == "__main__":
    main()